{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UjomtOyYPn-c_aJEWkBOmv8NP6OZnnlR",
      "authorship_tag": "ABX9TyNfOl/la9ImJj83ZzlQ/22R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myingineer/sentiment_analysis_with_NLP/blob/main/sentiment_analysis_with_NLP_A_S_Emeka_Akam_4242769.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YNudA8eUw2vg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the CSV"
      ],
      "metadata": {
        "id": "D4pR1yRf1aYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/Train.csv')\n",
        "test_df = pd.read_csv('/content/Test.csv')"
      ],
      "metadata": {
        "id": "izT2IjFQxfV3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O78iWM2-2kV-",
        "outputId": "865506a4-b866-4670-cc27-93e6a077f792"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRE-PROCESSING STEPS\n",
        "- Text Cleaning (Removing stop words, html e.t.c.)\n",
        "- Vectorization"
      ],
      "metadata": {
        "id": "jK2VnzRozHkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEXT CLEANING\n",
        "\n",
        "import re # to match regular expression\n",
        "import nltk # natural language toolkit\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.corpus import stopwords # words that are to be excluded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TlqH3-DxxOK",
        "outputId": "18f790c4-350c-4502-c3c0-12ea6cfd2c2e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "ncgQHbdjWjIU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_text(text):\n",
        "  '''\n",
        "    This function uses Regular Expressions to clean the sentences and apply stemming to the words\n",
        "    that are not part of the stop words in the library in order to reduce the size of the vocabulary.\n",
        "\n",
        "    Input: text (str)\n",
        "    Output: tokens (str)\n",
        "  '''\n",
        "\n",
        "  text = re.sub(r'<.*?>', '', text) # remove the HTML tags\n",
        "  text = re.sub(r'http\\S+|http\\s+', '', text)  # remove URLs (normal or incorrect urls with spaces)\n",
        "  text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # remove non-alphanumeric characters except spaces\n",
        "  text = re.sub(r'\\s+', ' ', text) # remove multiple whitespaces\n",
        "  text = text.strip() # trim leading white spaces\n",
        "\n",
        "  tokens = text.lower().split() # converts the texts to lowercase then splits each individual word\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words] # applies stemming to words that are not part of the stop words\n",
        "  return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "uVw3RFMuYkun"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_corpus(texts):\n",
        "  '''\n",
        "    This function takes a sentence and calls the clean_text function to clean it.\n",
        "    It then returns the cleaned sentence as a list\n",
        "\n",
        "    Input: sentence (str)\n",
        "    Output: cleaned_sentence (str)\n",
        "  '''\n",
        "  return [clean_text(text) for text in texts]"
      ],
      "metadata": {
        "id": "hVDsW2NufVLN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VECTORIZATION\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "def vectorize_texts(texts):\n",
        "  '''\n",
        "    This function vectorizes the texts and returns the shape of the\n",
        "    vectorized texts and the vectorizer object.\n",
        "\n",
        "    Input: texts (str)\n",
        "    Output: X (array), vectorizer (object)\n",
        "  '''\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  X = vectorizer.fit_transform(texts)\n",
        "  return X, vectorizer"
      ],
      "metadata": {
        "id": "BQhSyLQGiTJI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODELLING.\n",
        "In this section, we perform three things,\n",
        "  - train the model\n",
        "  - test the model\n",
        "  - evaluate the model"
      ],
      "metadata": {
        "id": "NmUzfmLTpqJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  Training dataset\n",
        "  The training dataset has a shape of (40000, 2)\n",
        "\"\"\"\n",
        "\n",
        "# PREPARING THE TRAIN DATASET\n",
        "X_train = train_df['text'] # text column as the input\n",
        "X_train = preprocess_corpus(X_train) # preprocessing the input\n",
        "X_train, vectorizer = vectorize_texts(X_train) # vectorizing the input\n",
        "\n",
        "y_train = train_df['label'] # labek column as the output"
      ],
      "metadata": {
        "id": "nvMJcJLmq4sP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  Testing data set\n",
        "  The testing dataset has a shape of (5000, 2)\n",
        "\"\"\"\n",
        "\n",
        "# PREPARING THE TESTING DATASET\n",
        "X_test = test_df['text'] # text column as the input\n",
        "X_test = preprocess_corpus(X_test) # preprocessing the input\n",
        "X_test = vectorizer.transform(X_test) # vectorizing the input\n",
        "\n",
        "y_test = test_df['label'] # label column as the output"
      ],
      "metadata": {
        "id": "EWLGdEOj9Tk2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would be usin different classification algorithms, to find out which one performs the best"
      ],
      "metadata": {
        "id": "MwiipvZG-PXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  The Algorithm used here is Linear Support Vector Classification\n",
        "'''\n",
        "\n",
        "# imports for linearSVC\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Training the model\n",
        "svm = LinearSVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Testing the model\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "\n",
        "# Evaluating the model\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print(\"The confusion matrix for the Linear SVC algorithm is: \" + \"\\n\" + str(confusion_matrix(y_test, y_pred_svm)))\n",
        "print(\"\\n\" + \"The classification report for the Linear SVC algorithm is: \" + \"\\n\" + str(classification_report(y_test, y_pred_svm)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpbnTzTzrfe1",
        "outputId": "f9a18cc1-80fa-44eb-aeab-faf8047e8300"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The confusion matrix for the Linear SVC algorithm is: \n",
            "[[2222  273]\n",
            " [ 233 2272]]\n",
            "\n",
            "The classification report for the Linear SVC algorithm is: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.89      0.90      2495\n",
            "           1       0.89      0.91      0.90      2505\n",
            "\n",
            "    accuracy                           0.90      5000\n",
            "   macro avg       0.90      0.90      0.90      5000\n",
            "weighted avg       0.90      0.90      0.90      5000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  The algorithm used here is Multinomial Naive Bayes\n",
        "\"\"\"\n",
        "\n",
        "# Imports for Multinomial NB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Training the model\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, y_train)\n",
        "\n",
        "# Testing the model\n",
        "y_pred_mnb = mnb.predict(X_test)\n",
        "\n",
        "# Evaluating the model\n",
        "print(\"The confusion matrix for the Multinomial NB algorithm is: \" + \"\\n\" + str(confusion_matrix(y_test, y_pred_mnb)))\n",
        "print(\"\\n\" + \"The classification report for the Multinomial NB algorithm is: \" + \"\\n\" + str(classification_report(y_test, y_pred_mnb)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaxnnZM0uoOo",
        "outputId": "19344667-b19e-4115-8e24-a75534ead1e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The confusion matrix for the Multinomial NB algorithm is: \n",
            "[[2196  299]\n",
            " [ 359 2146]]\n",
            "\n",
            "The classification report for the Multinomial NB algorithm is: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.88      0.87      2495\n",
            "           1       0.88      0.86      0.87      2505\n",
            "\n",
            "    accuracy                           0.87      5000\n",
            "   macro avg       0.87      0.87      0.87      5000\n",
            "weighted avg       0.87      0.87      0.87      5000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  The algorithm used here is Logistic Regression\n",
        "\"\"\"\n",
        "\n",
        "# Imports\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Training the model\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Testing the model\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "# Evaluating the model\n",
        "print(\"The confusion matrix for the Logistic Regression algorithm is: \" + \"\\n\" + str(confusion_matrix(y_test, y_pred_lr)))\n",
        "print(\"\\n\" + \"The classification report for the Logistic Regression algorithm is: \" + \"\\n\" + str(classification_report(y_test, y_pred_lr)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NipPeWUt9EPA",
        "outputId": "1bc63e3b-0672-4fbd-efae-6cc62a4dc701"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The confusion matrix for the Logistic Regression algorithm is: \n",
            "[[2212  283]\n",
            " [ 241 2264]]\n",
            "\n",
            "The classification report for the Logistic Regression algorithm is: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.89      0.89      2495\n",
            "           1       0.89      0.90      0.90      2505\n",
            "\n",
            "    accuracy                           0.90      5000\n",
            "   macro avg       0.90      0.90      0.90      5000\n",
            "weighted avg       0.90      0.90      0.90      5000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the pre-trained models\n",
        "joblib.dump(svm, 'svm_model.pkl')\n",
        "joblib.dump(mnb, 'mnb_model.pkl')\n",
        "joblib.dump(lr, 'lr_model.pkl')\n",
        "\n",
        "# Save the trained vectorizer\n",
        "joblib.dump(vectorizer, 'vectorizer.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s67jc5_ECL9L",
        "outputId": "d23bbe2c-17f5-41ae-ca86-82eaf55ca507"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use the model to make future predictions"
      ],
      "metadata": {
        "id": "KkOIZ9aXPKKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = joblib.load('/content/svm_model.pkl')\n",
        "\n",
        "# Load the vectorizer\n",
        "vectorizer = joblib.load('/content/vectorizer.pkl')\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    \"\"\"\n",
        "    Predict the sentiment of a given text using a pre-trained SVM model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to analyze.\n",
        "\n",
        "    Returns:\n",
        "        str: The predicted sentiment label ('positive' or 'negative').\n",
        "    \"\"\"\n",
        "    # Preprocess the input text\n",
        "    cleaned_text = preprocess_corpus([text])\n",
        "\n",
        "    # Transform the cleaned text using the same vectorizer used during training\n",
        "    vectorized_text = vectorizer.transform(cleaned_text)\n",
        "\n",
        "    # Make a prediction using the pre-trained SVM model\n",
        "    prediction = model.predict(vectorized_text)\n",
        "\n",
        "    # Map the prediction to sentiment labels\n",
        "    sentiment_label = 'positive' if prediction[0] == 1 else 'negative'\n",
        "\n",
        "    return sentiment_label"
      ],
      "metadata": {
        "id": "YVt8TBf4DnPE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = 'i am satisfied with the movie'\n",
        "print(predict_sentiment(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEfWQ8pdP8ce",
        "outputId": "b5596247-9ce3-436c-8495-66042cec2a69"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2tp_Nz-jP_a0"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}